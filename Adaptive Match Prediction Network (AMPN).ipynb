{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "39d37227-671f-4736-b452-25b705a818fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>round</th>\n",
       "      <th>day</th>\n",
       "      <th>venue</th>\n",
       "      <th>result</th>\n",
       "      <th>gf</th>\n",
       "      <th>ga</th>\n",
       "      <th>opponent</th>\n",
       "      <th>xg</th>\n",
       "      <th>xga</th>\n",
       "      <th>poss</th>\n",
       "      <th>sh</th>\n",
       "      <th>sot</th>\n",
       "      <th>fk</th>\n",
       "      <th>pk</th>\n",
       "      <th>pkatt</th>\n",
       "      <th>season</th>\n",
       "      <th>team</th>\n",
       "      <th>day_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-21</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Matchweek 2</td>\n",
       "      <td>Mon</td>\n",
       "      <td>Away</td>\n",
       "      <td>W</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wolves</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Matchweek 3</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Home</td>\n",
       "      <td>L</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Leicester City</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>72.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-03</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Matchweek 4</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Away</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Leeds United</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>48.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Matchweek 5</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Home</td>\n",
       "      <td>W</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>59.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Matchweek 6</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Away</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>Manchester City</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>2024-05-02</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Matchweek 26</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Home</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>37.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>2024-05-05</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Matchweek 36</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Home</td>\n",
       "      <td>W</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>69.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Matchweek 37</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Away</td>\n",
       "      <td>W</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Nott'ham Forest</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>2024-05-15</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Matchweek 34</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Away</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>45.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>2024-05-19</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Matchweek 38</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Home</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1617 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  time         round  day venue result   gf   ga  \\\n",
       "0     2020-09-21  20.0   Matchweek 2  Mon  Away      W  3.0  1.0   \n",
       "1     2020-09-27  16.0   Matchweek 3  Sun  Home      L  2.0  5.0   \n",
       "2     2020-10-03  17.0   Matchweek 4  Sat  Away      D  1.0  1.0   \n",
       "3     2020-10-17  17.0   Matchweek 5  Sat  Home      W  1.0  0.0   \n",
       "4     2020-10-24  12.0   Matchweek 6  Sat  Away      D  1.0  1.0   \n",
       "...          ...   ...           ...  ...   ...    ...  ...  ...   \n",
       "1612  2024-05-02  19.0  Matchweek 26  Thu  Home      W  2.0  0.0   \n",
       "1613  2024-05-05  14.0  Matchweek 36  Sun  Home      W  5.0  0.0   \n",
       "1614  2024-05-11  17.0  Matchweek 37  Sat  Away      W  3.0  2.0   \n",
       "1615  2024-05-15  19.0  Matchweek 34  Wed  Away      W  2.0  1.0   \n",
       "1616  2024-05-19  16.0  Matchweek 38  Sun  Home      W  2.0  1.0   \n",
       "\n",
       "             opponent   xg  xga  poss    sh   sot   fk   pk  pkatt  season  \\\n",
       "0              Wolves  1.9  1.0  65.0  13.0   8.0  2.0  1.0    1.0    2021   \n",
       "1      Leicester City  0.9  2.6  72.0  16.0   5.0  1.0  0.0    0.0    2021   \n",
       "2        Leeds United  1.5  1.7  48.0  23.0   1.0  1.0  0.0    0.0    2021   \n",
       "3             Arsenal  1.5  0.9  59.0  13.0   5.0  0.0  0.0    0.0    2021   \n",
       "4            West Ham  1.1  0.5  70.0  14.0   7.0  1.0  0.0    0.0    2021   \n",
       "...               ...  ...  ...   ...   ...   ...  ...  ...    ...     ...   \n",
       "1612        Tottenham  2.2  1.6  37.0  16.0   4.0  2.0  0.0    0.0    2023   \n",
       "1613         West Ham  4.1  0.9  69.0  25.0  14.0  1.0  0.0    0.0    2023   \n",
       "1614  Nott'ham Forest  1.6  1.5  67.0  12.0   5.0  1.0  0.0    0.0    2023   \n",
       "1615         Brighton  1.5  1.3  45.0  14.0   6.0  2.0  0.0    0.0    2023   \n",
       "1616      Bournemouth  1.1  2.2  61.0  16.0   6.0  0.0  0.0    0.0    2023   \n",
       "\n",
       "                 team  day_code  \n",
       "0     Manchester City         0  \n",
       "1     Manchester City         6  \n",
       "2     Manchester City         5  \n",
       "3     Manchester City         5  \n",
       "4     Manchester City         5  \n",
       "...               ...       ...  \n",
       "1612          Chelsea         3  \n",
       "1613          Chelsea         6  \n",
       "1614          Chelsea         5  \n",
       "1615          Chelsea         2  \n",
       "1616          Chelsea         6  \n",
       "\n",
       "[1617 rows x 20 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and inspect the dataset\n",
    "data = pd.read_csv('Match_Result_Data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f423f134-d868-461b-b5ef-22ebfda9853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1617 entries, 0 to 1616\n",
      "Data columns (total 20 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   date      1617 non-null   object \n",
      " 1   time      1617 non-null   float64\n",
      " 2   round     1617 non-null   object \n",
      " 3   day       1617 non-null   object \n",
      " 4   venue     1617 non-null   object \n",
      " 5   result    1617 non-null   object \n",
      " 6   gf        1617 non-null   float64\n",
      " 7   ga        1617 non-null   float64\n",
      " 8   opponent  1617 non-null   object \n",
      " 9   xg        1617 non-null   float64\n",
      " 10  xga       1617 non-null   float64\n",
      " 11  poss      1617 non-null   float64\n",
      " 12  sh        1617 non-null   float64\n",
      " 13  sot       1617 non-null   float64\n",
      " 14  fk        1617 non-null   float64\n",
      " 15  pk        1617 non-null   float64\n",
      " 16  pkatt     1617 non-null   float64\n",
      " 17  season    1617 non-null   int64  \n",
      " 18  team      1617 non-null   object \n",
      " 19  day_code  1617 non-null   int64  \n",
      "dtypes: float64(11), int64(2), object(7)\n",
      "memory usage: 252.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d17fb2ad-5b9f-418e-823b-87845148d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: Data Preprocessing ###\n",
    "\n",
    "# Encode categorical columns to numeric values\n",
    "label_encoders = {}\n",
    "for column in ['venue', 'result', 'opponent', 'day']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad5090-9112-479a-810d-b048121020ae",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "We are using LabelEncoder to convert categorical columns ('venue', 'result', 'opponent', 'day') into numeric values. This is necessary because most machine learning models require numeric inputs.\n",
    "A dictionary (label_encoders) stores each encoder to access the mappings later if needed (like for inverse transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "79db1f10-7026-45e2-aeea-2d177bff1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map 'result' column to a 3-class target (e.g., Win=1, Draw=0, Loss=2)\n",
    "data['result'] = data['result'].map({\n",
    "    1: 1,  # Win\n",
    "    0: 0,  # Draw\n",
    "    2: 2 # Loss\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467287c-b058-46ac-90cf-ecacfa61cee5",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The result column is remapped to define the target variable:\n",
    "1: Win\n",
    "0: Draw\n",
    "2: Loss\n",
    "This step ensures that the results are converted to a three-class target for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4f8f9680-20ae-49ea-89b5-e049d75e3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "data.drop(columns=['date', 'round', 'season', 'team'], inplace=True)\n",
    "\n",
    "# Fill missing values with 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4db07f-b772-4432-9d60-ab0fb07202a3",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Irrelevant columns ('date', 'round', 'season', 'team') are dropped since they do not provide predictive value in the model or can cause data leakage (e.g., specific match dates influencing predictions).\n",
    "\n",
    "Any missing values in the dataset are filled with 0. This is a simple approach to handle missing data, though you might want to consider more sophisticated imputation methods if missing values are common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5ea05dad-f1cb-43e0-9496-c1f54f5c5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Feature Engineering ###\n",
    "\n",
    "# Calculate rolling averages for critical performance metrics\n",
    "def calculate_rolling_avg(df, column, window=3):\n",
    "    return df[column].rolling(window=window, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe58e6b-d47a-4c5c-8284-8e20e5743cb6",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "This defines a helper function calculate_rolling_avg to compute rolling averages. A rolling average smooths data over a specific window (in this case, a default of 3 games). This helps capture recent trends in a team's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0cf11-59c7-463b-ac18-7f8af5ce485a",
   "metadata": {},
   "source": [
    "Cause of using Rolling Average?\n",
    "\n",
    "Capture Recent Trends:\n",
    "In sports, a team's recent performance often has a greater impact on future outcomes than older performances. By using a rolling average, you can capture how a team's form is evolving over time.\n",
    "For example, if you calculate the rolling average of goals_for (gf) over the last 3 matches, it will give more emphasis to how the team has been scoring recently, rather than taking all past games equally.\n",
    "\n",
    "Smooth Out Fluctuations:\n",
    "Performance metrics like goals, shots, or possession may fluctuate from game to game. A rolling average helps to smooth out these short-term variations, making it easier for your model to detect long-term trends.\n",
    "For instance, if a team scores unusually high or low in one match, the rolling average tempers the effect by considering a few more matches.\n",
    "\n",
    "Reduce Noise:\n",
    "Rolling averages help reduce noise (random high or low spikes in data), which can be beneficial for the model by reducing sensitivity to outliers or anomalies in the data.\n",
    "In the context of football, one-off performances (like scoring a lot of goals in a single match) might not be reflective of a team's overall trend. The rolling average mitigates the influence of such outliers.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Window Size: The size of the window (default = 3) determines how many recent matches are considered for the rolling average. In your case, you're considering the last 3 games, which is a reasonable window to assess recent form without overfitting to short-term noise.\n",
    "\n",
    "Min Periods: min_periods=1 ensures that even if there aren't enough past matches to fill the window (e.g., fewer than 3 games), the rolling average will still be calculated using whatever data is available.\n",
    "\n",
    "Temporal Feature: Rolling averages introduce a temporal aspect into your model by providing features that account for how teams have performed over time, which can be crucial for predicting future outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e626a098-5cc9-4496-9fbd-3018545563c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['gf', 'ga', 'poss', 'xg', 'xga', 'sh', 'sot']:\n",
    "    data[f'{col}_roll_avg'] = calculate_rolling_avg(data, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0ffa5-1074-416b-8ee2-2929848281fd",
   "metadata": {},
   "source": [
    "This loop applies the calculate_rolling_avg function to important performance metrics ('gf', 'ga', 'poss', 'xg', 'xga', 'sh', 'sot') and creates new columns ('{col}_roll_avg') for the rolling averages.\n",
    "These new features represent the average of the team's performance metrics over the last 3 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "252e810a-fb19-461c-b2ff-a5b65efb83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'day_code' is integer\n",
    "data['day_code'] = data['day_code'].astype(int)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(columns=['result'])\n",
    "y = data['result']\n",
    "#Here, the dataset is split into features (X) and target (y).\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#standerization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#The data is split into training and test sets using train_test_split.\n",
    "#80% of the data is used for training, and 20% is reserved for testing the model's performance. A fixed random_state ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "96d510bd-faa4-4d47-8f5f-0ce01e73e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Building the Adaptive Match Prediction Network (AMPN) ###\n",
    "\n",
    "# Define a custom Influence Layer\n",
    "class InfluenceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(InfluenceLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.influence_weights = self.add_weight(\n",
    "            shape=(input_shape[-1], self.output_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        influence_score = tf.matmul(inputs, self.influence_weights)\n",
    "        return tf.nn.relu(influence_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdc5ff-a5b3-4013-a0c8-3b6cc2e34470",
   "metadata": {},
   "source": [
    "Purpose of the InfluenceLayer\n",
    "\n",
    "The key motivation behind using the InfluenceLayer is to capture the relative importance or influence of different feature groups in predicting the match outcome. In your football match prediction model, you have different types of features, such as offensive features (e.g., goals scored, shots on target), defensive features (e.g., goals conceded, possession), and match-specific features (e.g., venue, day). Each of these feature sets may have different levels of importance in determining whether a team wins, loses, or draws a match. The InfluenceLayer is designed to quantify and capture this importance dynamically during training.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Layer Initialization: The constructor (__init__) defines the basic configuration of the layer. Here, the output_dim argument specifies how many output dimensions the layer will have.This could represent the influence scores or weights that will be applied to the feature group during training.\n",
    "\n",
    "The super() call ensures that the parent class (tf.keras.layers.Layer) is properly initialized.\n",
    "\n",
    "Output Dimensionality (output_dim): This parameter represents the number of neurons or outputs produced by the layer. In your case, this is typically set to 1, meaning the layer will produce a single influence score (or value) for each feature group. This score will represent how important or influential the feature group is for predicting match outcomes.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Why Use an InfluenceLayer?\n",
    "\n",
    "Capturing Feature Importance: In traditional neural networks, each feature is treated equally, and the network learns which features are important based on the weights. However, in your case, you know that some features (such as goals scored or shots on target) have a different level of importance than others. This layer is designed to learn how much influence each feature group should have in making predictions.\n",
    "\n",
    "Separation of Feature Groups: By using separate InfluenceLayers for different groups (offensive, defensive, match-related features), the network can learn independent importance scores for each group, making the model more interpretable and tailored to specific football-related metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7bfd0-9a4b-4218-ae26-7fe4dfbb471b",
   "metadata": {},
   "source": [
    "Building the Layer (build method):\n",
    "\n",
    "The build() method is called when the layer is first created, and this is where you define the trainable weights of the layer.\n",
    "\n",
    "add_weight(): This method creates a trainable weight matrix (called influence_weights) that will be used to calculate influence scores for the features.\n",
    "\n",
    "shape=(input_shape[-1], self.output_dim): This shape specifies how the weights will map the input features to the output. Here, input_shape[-1] refers to the number of input features (i.e., the number of features in a feature group). The output_dim is typically 1, so the layer will produce one influence score for the group of input features.\n",
    "\n",
    "initializer='random_normal': This specifies how the weights will be initialized at the start of training. random_normal means that the weights will be initialized with small random values drawn from a normal distribution.\n",
    "\n",
    "trainable=True: This indicates that the weights will be adjusted during the training process via backpropagation. As the model trains, these weights will be optimized to reflect the true influence of each feature group on the prediction task.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Why Define Weights for Feature Groups?\n",
    "\n",
    "Learning Influence Dynamically: Instead of manually assigning importance to each feature group, the network will learn these values during training. This allows the model to adapt to the data and capture which features (or feature groups) are truly predictive of match outcomes.\n",
    "\n",
    "Interpretability: After training, these weights can be interpreted to understand which feature groups are more important. For example, if the weights for offensive features are higher, it means that offensive metrics have a stronger influence on match outcomes compared to other metrics like defensive ones.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "In TensorFlow/Keras, the input_shape argument represents the shape of the input data that is passed into a neural network layer. It typically comes in the form of a tuple. Here's what the components mean:\n",
    "\n",
    "input_shape: This is the shape of the input data, typically given as (batch_size, feature_dim).\n",
    "\n",
    "batch_size: The number of samples in a batch of input data. It can vary from batch to batch and is usually not specified in input_shape (hence it is sometimes omitted or set to None).\n",
    "\n",
    "feature_dim: The number of features for each sample. This is the important part when determining how many features are being passed into the layer.\n",
    "\n",
    "What Does input_shape[-1] Mean?\n",
    "When you see input_shape[-1], it means the last dimension of the input shape. In the context of neural networks, this refers to the number of features for each sample.\n",
    "\n",
    "For example:\n",
    "\n",
    "If input_shape is (32, 10), where: 32 is the batch size (number of samples in the batch),10 is the number of features per sample.\n",
    "In this case, input_shape[-1] would be 10, which represents the number of input features.\n",
    "\n",
    "Why -1 in input_shape[-1]?\n",
    "The use of -1 here refers to the last element of the shape tuple. In Python (and many programming languages), you can index from the end of a list or tuple using negative indexing:\n",
    "\n",
    "input_shape[-1] gives you the last dimension (the number of features).If input_shape = (32, 10), then input_shape[-1] = 10 (the number of features).\n",
    "This is convenient because it lets you easily access the feature dimension without having to explicitly write out the shape (especially useful when you don't need to know the batch size).\n",
    "\n",
    "=========================================================================================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9351e4f-b9df-463d-adce-9643335d8ccb",
   "metadata": {},
   "source": [
    "Forward Pass (call method):\n",
    "\n",
    "The call() method is where the actual forward pass of the layer happens. This is where the layer takes the input data and produces the output.\n",
    "\n",
    "tf.matmul(): This function performs matrix multiplication between the input features (inputs) and the influence weights (self.influence_weights). The result is an \"influence score\" for each input feature group.\n",
    "\n",
    "In that case, the input to this layer would be a subset of features (e.g., offensive, defensive metrics), and the output would be a single influence score that represents the collective influence of those features.\n",
    "tf.nn.relu(): This applies a non-linear activation function called ReLU (Rectified Linear Unit). ReLU is commonly used in neural networks because it introduces non-linearity, which helps the model capture complex relationships in the data.\n",
    "\n",
    "        Why ReLU: ReLU ensures that negative influence scores are set to zero, which makes sense in this context. You likely want to discard negative influence scores since negative contributions from a feature group (such as poor offensive performance) should not contribute to the final influence in the same way that positive scores do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a7f6f-4779-4de4-8c2b-0af2cbbe31e4",
   "metadata": {},
   "source": [
    "Workflow of the InfluenceLayer in Your Model:\n",
    "\n",
    "Input Layer: The raw input features are passed into the model (e.g., offensive metrics like goals scored, shots on target, etc.).\n",
    "Feature Grouping: You group features into logical sets (e.g., offensive, defensive, match-related).\n",
    "Influence Layers: Each feature group is passed through a separate InfluenceLayer, which calculates an influence score based on a learned weight matrix.\n",
    "Combination: The influence scores from all feature groups are combined in the meta-combination layer to generate a final prediction of the match outcome.\n",
    "\n",
    "Summary of the Key Benefits:\n",
    "\n",
    "Feature-specific weight learning: The InfluenceLayer learns a specific weight for each feature group, dynamically adjusting how much influence each group should have on the match outcome.\n",
    "Smooth integration: It fits seamlessly into the larger neural network, contributing to the overall model's ability to make predictions based on the learned influences.\n",
    "Greater control and flexibility: You can use this layer to create a more interpretable and customizable model that reflects the importance of different aspects of football matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "af207a62-43db-44e7-83f0-f51001907ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model input layer\n",
    "input_layer = Input(shape=(X_train.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba013e-f12a-40c8-83f9-6609bde7dceb",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The input layer for the Keras model is defined, where X_train.shape[1] specifies the number of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f6d16efd-6633-444e-8fe3-81e769af41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups and apply Influence Layers\n",
    "#This helper function is designed to:\n",
    "#Select specific groups of features from the input data.\n",
    "#Apply the custom InfluenceLayer to these selected features to calculate an \"influence score\" based on those features.\n",
    "\n",
    "def get_influence_layer(inputs, features):\n",
    "    indices = [X.columns.get_loc(f) for f in features]\n",
    "    feature_input = Lambda(lambda x: tf.gather(x, indices, axis=1))(inputs)\n",
    "    return InfluenceLayer(1)(feature_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1863fb-9223-4c54-8101-560181e18e0f",
   "metadata": {},
   "source": [
    "Function Definition:\n",
    "\n",
    "inputs: This is the full input layer (or data) that contains all the features for the model.\n",
    "features: A list of feature names that belong to a specific group. For example, this could be a group of offensive features like ['goals', 'shots', 'assists'].\n",
    "The purpose of this function is to extract the specific feature group from the input and then apply the InfluenceLayer on this feature group.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Finding Feature Indices:\n",
    "\n",
    "\n",
    "indices = [X.columns.get_loc(f) for f in features]\n",
    "\n",
    "This line uses list comprehension to get the indices (column positions) of the selected feature names from the DataFrame X (the full feature matrix).\n",
    "\n",
    "X.columns.get_loc(f) finds the index (position) of feature f in the list of columns of X. for f in features: It loops through all the feature names in the features list.\n",
    "Example: If features = ['goals', 'shots', 'assists'] and X has columns in this order ['venue', 'day_code', 'goals', 'shots', 'assists', 'xG'], then:\n",
    "\n",
    "X.columns.get_loc('goals') would return 2 (because 'goals' is the third column in the DataFrame),\n",
    "X.columns.get_loc('shots') would return 3,\n",
    "X.columns.get_loc('assists') would return 4.\n",
    "\n",
    "After this line runs, the variable indices will store the list [2, 3, 4], which corresponds to the positions of the goals, shots, and assists columns in the dataset.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Selecting Relevant Features Using tf.gather:\n",
    "\n",
    "feature_input = Lambda(lambda x: tf.gather(x, indices, axis=1))(inputs)\n",
    "\n",
    "This line is used to extract the specific columns (features) from the input layer (inputs) using the indices identified earlier.\n",
    "\n",
    "Here's what's happening:\n",
    "\n",
    "Lambda(lambda x: ...): The Lambda layer is a simple way to wrap any custom TensorFlow function into a Keras model. Here, it's wrapping the tf.gather operation.\n",
    "tf.gather(x, indices, axis=1): This is a TensorFlow operation that selects specific columns from the input x based on the indices.\n",
    "x represents the input data (i.e., the input layer).\n",
    "indices is the list of feature positions that we want to select (e.g., [2, 3, 4]).\n",
    "axis=1 means we are selecting columns (features) from the input. axis=0 refers to rows, and axis=1 refers to columns.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Applying the InfluenceLayer:\n",
    "\n",
    "return InfluenceLayer(1)(feature_input)\n",
    "\n",
    "This line applies the custom InfluenceLayer to the selected features (feature_input), and it will return the output of this layer, which is the influence score for that group of features.\n",
    "\n",
    "InfluenceLayer(1): This initializes the custom InfluenceLayer with an output dimension of 1. This means the layer will output a single score representing the \"influence\" of the feature group on the model.\n",
    "(feature_input): This passes the extracted feature data (e.g., goals, shots, assists) into the InfluenceLayer.\n",
    "The InfluenceLayer will calculate a weighted influence score based on the input features by multiplying the features by the learned influence_weights, as defined in the custom layer. This output will then be used by the model as part of the decision-making process.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Summary of the Workflow:\n",
    "\n",
    "inputs: This is the full input data, containing all features.\n",
    "features: A list of specific features (e.g., offensive or defensive features) you want to isolate.\n",
    "indices: The function first finds the positions of these features in the full dataset.\n",
    "tf.gather: It then extracts only the columns (features) corresponding to those positions.\n",
    "InfluenceLayer: Finally, it applies the custom InfluenceLayer to compute the influence score for the selected feature group. This score reflects how much influence the feature group (e.g., offensive features) has on the overall prediction (e.g., predicting win/loss).\n",
    "\n",
    "Why Use This?\n",
    "\n",
    "Feature Grouping: This allows you to isolate different types of features (e.g., offensive vs. defensive) and calculate their individual influence on the overall prediction.\n",
    "Modular Influence Calculation: By using the InfluenceLayer, you're able to compute influence scores separately for each feature group and combine them later. This helps in better interpreting which features or groups of features are most important in determining the match outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6b3a653b-ecaa-440a-a66d-158635f31fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define influence layers for different feature groups\n",
    "offensive_influence = get_influence_layer(input_layer, ['gf', 'sh', 'sot', 'xg', 'gf_roll_avg'])\n",
    "defensive_influence = get_influence_layer(input_layer, ['ga', 'xga', 'poss', 'ga_roll_avg'])\n",
    "match_influence = get_influence_layer(input_layer, ['venue', 'day_code', 'time', 'poss_roll_avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ffad5-9351-4982-a9e7-11943b993617",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Three influence layers are created for different feature groups:\n",
    "Offensive Influence: Metrics like goals, shots, and xG.\n",
    "Defensive Influence: Metrics like goals conceded, xGA, and possession.\n",
    "Match Influence: Contextual factors like venue, day of the match, and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fd1d5dc6-751d-4d5a-bef5-ba656020724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-Combination Layer (combine influences)\n",
    "combined_influence = tf.keras.layers.Concatenate()([offensive_influence, defensive_influence, match_influence])\n",
    "meta_combination = Dense(16, activation='relu')(combined_influence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd618aa5-0544-4cb0-84c7-ed1cccfaf5f1",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The outputs from the three influence layers are concatenated into a single tensor using Concatenate. This combines the influences from offensive, defensive, and match-specific features.\n",
    "\n",
    "A dense (fully connected) layer with 16 neurons is applied to the combined influences to further process the aggregated features. ReLU activation is used to introduce non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "987cf837-e883-40e1-849f-ec83fdfa1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer for regularization\n",
    "dropout = Dropout(0.3)(meta_combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded2fbc-6007-47bb-9192-25a9e8c3ffcb",
   "metadata": {},
   "source": [
    "A Dropout layer is added to prevent overfitting by randomly setting 30% of the neurons to zero during training. This helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4ef3d830-d439-4b62-9b5f-1b4dbc13c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction layer (output with 3 classes for Win, Draw, Loss)\n",
    "output = Dense(3, activation='softmax')(dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773fab1-d90b-4054-998a-fbd1a33a457f",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The output layer has 3 neurons (for the three possible outcomes: Win, Draw, Loss).\n",
    "A softmax activation is applied to output a probability distribution over the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d77cb8df-28ef-4198-b464-a97fadf68764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the model\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9882c2-cb3d-41d7-9c77-3cff23373454",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The Keras model is created, specifying the input and output layers.\n",
    "The Adam optimizer is used for efficient gradient-based optimization.\n",
    "The loss function is sparse_categorical_crossentropy, suitable for multi-class classification with integer labels.\n",
    "accuracy is included as a metric to evaluate model performance.\n",
    "\n",
    "=========================================================================================================================================================\n",
    "\n",
    "Efficient gradient-based optimization refers to the use of techniques that improve the process of finding the optimal parameters (weights) for a machine learning model by minimizing a specific loss function. Gradient-based methods rely on calculating the gradients (or slopes) of the loss function with respect to the model's parameters and using these gradients to adjust the parameters in a direction that minimizes the loss.\n",
    "\n",
    "In this context, efficiency means using methods that allow the optimization process to converge more quickly, use less computational resources, or both, while ensuring good performance and stability.\n",
    "\n",
    "Key Concepts:\n",
    "Gradient Descent: Gradient descent is a common optimization technique where the model's parameters are adjusted iteratively to minimize the loss function. It works by computing the gradient of the loss function with respect to the model parameters and updating the parameters in the direction that decreases the loss.\n",
    "\n",
    "The gradient is the vector of partial derivatives of the loss function with respect to each parameter.\n",
    "By following the negative of this gradient, the model moves toward a local (or global) minimum of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ed4abb57-0fde-414f-a757-9494fcd132c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3163 - loss: 1.1024 - val_accuracy: 0.4131 - val_loss: 1.0873\n",
      "Epoch 2/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4432 - loss: 1.0813 - val_accuracy: 0.4208 - val_loss: 1.0726\n",
      "Epoch 3/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4994 - loss: 1.0606 - val_accuracy: 0.5212 - val_loss: 1.0494\n",
      "Epoch 4/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5205 - loss: 1.0293 - val_accuracy: 0.5907 - val_loss: 1.0096\n",
      "Epoch 5/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5605 - loss: 0.9818 - val_accuracy: 0.6216 - val_loss: 0.9605\n",
      "Epoch 6/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6261 - loss: 0.9284 - val_accuracy: 0.6525 - val_loss: 0.9112\n",
      "Epoch 7/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.8803 - val_accuracy: 0.6602 - val_loss: 0.8647\n",
      "Epoch 8/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6701 - loss: 0.8134 - val_accuracy: 0.6718 - val_loss: 0.8245\n",
      "Epoch 9/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6570 - loss: 0.7997 - val_accuracy: 0.6757 - val_loss: 0.7909\n",
      "Epoch 10/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6761 - loss: 0.7768 - val_accuracy: 0.6718 - val_loss: 0.7664\n",
      "Epoch 11/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6368 - loss: 0.7788 - val_accuracy: 0.6757 - val_loss: 0.7447\n",
      "Epoch 12/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6860 - loss: 0.7216 - val_accuracy: 0.6795 - val_loss: 0.7270\n",
      "Epoch 13/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6837 - loss: 0.6989 - val_accuracy: 0.6757 - val_loss: 0.7067\n",
      "Epoch 14/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7077 - loss: 0.6647 - val_accuracy: 0.6718 - val_loss: 0.6903\n",
      "Epoch 15/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6774 - loss: 0.6796 - val_accuracy: 0.6680 - val_loss: 0.6753\n",
      "Epoch 16/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6936 - loss: 0.6571 - val_accuracy: 0.6680 - val_loss: 0.6614\n",
      "Epoch 17/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7167 - loss: 0.6367 - val_accuracy: 0.6718 - val_loss: 0.6477\n",
      "Epoch 18/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7186 - loss: 0.6009 - val_accuracy: 0.6757 - val_loss: 0.6354\n",
      "Epoch 19/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6834 - loss: 0.6420 - val_accuracy: 0.6834 - val_loss: 0.6253\n",
      "Epoch 20/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6962 - loss: 0.6086 - val_accuracy: 0.6834 - val_loss: 0.6142\n",
      "Epoch 21/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 0.6201 - val_accuracy: 0.6834 - val_loss: 0.6041\n",
      "Epoch 22/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6919 - loss: 0.5882 - val_accuracy: 0.6834 - val_loss: 0.5933\n",
      "Epoch 23/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6988 - loss: 0.6002 - val_accuracy: 0.6834 - val_loss: 0.5863\n",
      "Epoch 24/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7183 - loss: 0.5696 - val_accuracy: 0.7297 - val_loss: 0.5791\n",
      "Epoch 25/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7129 - loss: 0.5951 - val_accuracy: 0.7375 - val_loss: 0.5688\n",
      "Epoch 26/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7145 - loss: 0.5696 - val_accuracy: 0.7336 - val_loss: 0.5602\n",
      "Epoch 27/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7171 - loss: 0.5660 - val_accuracy: 0.7529 - val_loss: 0.5527\n",
      "Epoch 28/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7182 - loss: 0.5603 - val_accuracy: 0.7529 - val_loss: 0.5475\n",
      "Epoch 29/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7444 - loss: 0.5357 - val_accuracy: 0.7606 - val_loss: 0.5415\n",
      "Epoch 30/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7309 - loss: 0.5506 - val_accuracy: 0.7568 - val_loss: 0.5322\n",
      "Epoch 31/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7762 - loss: 0.5430 - val_accuracy: 0.7606 - val_loss: 0.5213\n",
      "Epoch 32/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7621 - loss: 0.5177 - val_accuracy: 0.7568 - val_loss: 0.5153\n",
      "Epoch 33/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7492 - loss: 0.5302 - val_accuracy: 0.7683 - val_loss: 0.5081\n",
      "Epoch 34/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7653 - loss: 0.5189 - val_accuracy: 0.7722 - val_loss: 0.4989\n",
      "Epoch 35/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7720 - loss: 0.4856 - val_accuracy: 0.7722 - val_loss: 0.4892\n",
      "Epoch 36/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7997 - loss: 0.4963 - val_accuracy: 0.7761 - val_loss: 0.4847\n",
      "Epoch 37/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.5105 - val_accuracy: 0.7761 - val_loss: 0.4779\n",
      "Epoch 38/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.4976 - val_accuracy: 0.7761 - val_loss: 0.4701\n",
      "Epoch 39/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7971 - loss: 0.4943 - val_accuracy: 0.7799 - val_loss: 0.4645\n",
      "Epoch 40/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.5006 - val_accuracy: 0.7954 - val_loss: 0.4544\n",
      "Epoch 41/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8034 - loss: 0.4633 - val_accuracy: 0.8031 - val_loss: 0.4495\n",
      "Epoch 42/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8228 - loss: 0.4517 - val_accuracy: 0.8031 - val_loss: 0.4431\n",
      "Epoch 43/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8003 - loss: 0.4741 - val_accuracy: 0.8031 - val_loss: 0.4375\n",
      "Epoch 44/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8103 - loss: 0.4696 - val_accuracy: 0.8031 - val_loss: 0.4349\n",
      "Epoch 45/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7936 - loss: 0.4533 - val_accuracy: 0.8069 - val_loss: 0.4312\n",
      "Epoch 46/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7919 - loss: 0.4583 - val_accuracy: 0.8108 - val_loss: 0.4272\n",
      "Epoch 47/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8048 - loss: 0.4561 - val_accuracy: 0.8108 - val_loss: 0.4227\n",
      "Epoch 48/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7988 - loss: 0.4652 - val_accuracy: 0.8185 - val_loss: 0.4185\n",
      "Epoch 49/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7885 - loss: 0.4623 - val_accuracy: 0.8185 - val_loss: 0.4160\n",
      "Epoch 50/50\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7945 - loss: 0.4764 - val_accuracy: 0.8185 - val_loss: 0.4111\n"
     ]
    }
   ],
   "source": [
    "### Step 4: Training the Model ###\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a042976-1f29-4517-b27d-6cbaf1ceed7e",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The model is trained for 50 epochs with a batch size of 32.\n",
    "20% of the training data is used for validation to monitor the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "902db219-b3bb-4307-a85d-2f782f6edb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "### Step 5: Evaluating the Model ###\n",
    "\n",
    "# Predict and evaluate performance\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "accuracy = accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a265e7f-86a4-4cfb-98ff-2e4b0255a30b",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "After training, the model is evaluated on the test set to assess its performance.\n",
    "The test accuracy is printed to show how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "62ef5ad1-0519-4286-8972-86399a4e4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Model Performance Metrics:\n",
      "Accuracy: 0.7962962962962963\n",
      "\n",
      "Unique classes in y_test: [0 1 2]\n",
      "Unique classes in y_pred: [0 1 2]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Loss       0.98      0.82      0.89       125\n",
      "        Draw       0.53      0.90      0.67        73\n",
      "         Win       0.95      0.71      0.81       126\n",
      "\n",
      "    accuracy                           0.80       324\n",
      "   macro avg       0.82      0.81      0.79       324\n",
      "weighted avg       0.87      0.80      0.81       324\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict and evaluate performance\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print model performance metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Check unique classes in y_test and y_pred\n",
    "unique_classes_test = np.unique(y_test)\n",
    "unique_classes_pred = np.unique(y_pred)\n",
    "\n",
    "print(\"\\nUnique classes in y_test:\", unique_classes_test)\n",
    "print(\"Unique classes in y_pred:\", unique_classes_pred)\n",
    "\n",
    "# Print classification report with appropriate labels\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Loss', 'Draw', 'Win'], labels=[2, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1a04ef3d-105f-4cde-8275-272cfd2c1af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "2    539\n",
      "1    467\n",
      "0    287\n",
      "Name: count, dtype: int64\n",
      "result\n",
      "1    126\n",
      "2    125\n",
      "0     73\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a689c4e-4543-4fcd-b23c-5b49c37344ba",
   "metadata": {},
   "source": [
    "To tailor the Adaptive Match Prediction Network (AMPN) to your dataset, I’ll first examine its columns to understand the features available. Once that’s clarified, I’ll integrate the specific data attributes into each step of the algorithm and describe how the model processes each column to produce predictions.\n",
    "\n",
    "Loading and Exploring the Dataset\n",
    "I'll load the dataset to understand the specific columns and values.\n",
    "\n",
    "The dataset includes the following columns:\n",
    "\n",
    "Match Information: date, time, round, day, venue (Home/Away), opponent, season, team, day_code.\n",
    "Performance Metrics: gf (goals for), ga (goals against), xg (expected goals), xga (expected goals against), poss (possession), sh (shots), sot (shots on target), fk (free kicks), pk (penalty kicks scored), pkatt (penalty kick attempts).\n",
    "Target Variable: result (win: \"W\", loss: \"L\", draw: \"D\").\n",
    "The objective is to predict result based on these features. I’ll now define the Adaptive Match Prediction Network (AMPN) algorithm in detail, integrating these dataset columns.\n",
    "\n",
    "Algorithm: Adaptive Match Prediction Network (AMPN)\n",
    "Step-by-Step Breakdown\n",
    "1. Data Preprocessing\n",
    "Handling Categorical Data:\n",
    "\n",
    "Convert venue and result (target variable) into numerical labels for model processing. Assign venue values (e.g., Home = 1, Away = 0) and convert result labels to numerical values (Win = 1, Draw = 0, Loss = 2).\n",
    "Feature Engineering:\n",
    "\n",
    "Rolling Averages: Calculate rolling averages of gf, ga, poss, xg, xga, and sot over the past 3, 5, and 10 matches for each team.\n",
    "Recent Form Indicator: Create a recent form metric as the sum of results over the past few games (e.g., last 3 or 5 matches), giving more weight to recent games.\n",
    "Opponent Strength Indicator: Calculate average performance metrics of the opponent team across the season (or most recent games) to gauge difficulty level.\n",
    "Day and Time Encoding: Encode day and time features to represent temporal patterns.\n",
    "2. Algorithm Design\n",
    "The AMPN architecture will include three main layers: Segmentation Layer (SL), Influence Layers (IL), and Meta-Combination Layer (MCL), followed by a Prediction Layer (PL).\n",
    "\n",
    "A. Segmentation Layer (SL)\n",
    "Purpose: Group data into segments based on the similarity of match conditions.\n",
    "Implementation:\n",
    "Segment matches by conditions such as venue (Home, Away), and recent form (e.g., last 5 games’ outcomes).\n",
    "Each segment is treated as a mini-batch during training, helping the model learn different patterns based on contextual match conditions.\n",
    "B. Influence Layers (IL)\n",
    "Purpose: Determine the influence strength of each feature group on match outcomes.\n",
    "Implementation:\n",
    "Divide features into groups such as offensive performance (gf, sh, sot, xg), defensive performance (ga, xga, poss), and match-specific indicators (venue, recent form, and opponent strength).\n",
    "Calculate Influence Scores: Each IL produces an influence score representing how strongly its feature group correlates with outcomes.\n",
    "Dynamic Influence Updating:\n",
    "The model refines influence scores for each feature through iterative adjustments based on a custom loss function, emphasizing feature influence consistency within similar matches.\n",
    "The influence scores are stored for each feature in each segment.\n",
    "C. Meta-Combination Layer (MCL)\n",
    "Purpose: Combine influence scores from ILs across all segments.\n",
    "Implementation:\n",
    "Use a weighted averaging of influence scores, where each weight is dynamically assigned based on recent match performance.\n",
    "For each match, MCL calculates a feature-weighted score by combining influence scores, considering segment relevance.\n",
    "Include a regularization penalty in MCL to prevent over-reliance on specific feature groups, encouraging balanced learning across all features.\n",
    "D. Prediction Layer (PL)\n",
    "Purpose: Generate the final prediction.\n",
    "Implementation:\n",
    "PL aggregates the weighted scores from MCL for each segment, producing a final score for the prediction.\n",
    "To avoid overfitting, PL uses a \"dropout\" strategy on minor features, randomly ignoring them during training.\n",
    "The final output will be a probability for each class (win, loss, draw), which will be converted into the predicted match result.\n",
    "3. Training and Validation\n",
    "Loss Function: Define a custom loss function that minimizes prediction error while also penalizing inconsistencies in influence scores across segments.\n",
    "Training Strategy: Use mini-batch gradient descent with segment-based batches. The ILs adjust influence scores iteratively, while the MCL aggregates scores to enhance robustness across different match contexts.\n",
    "Validation: Use cross-segment validation to ensure that the model performs well across diverse match conditions, such as different venues and against varied opponents.\n",
    "4. Evaluation Metrics\n",
    "Accuracy: Calculate the percentage of correct predictions.\n",
    "Precision, Recall, and F1 Score: Especially useful if the dataset has class imbalances (e.g., more wins than losses).\n",
    "Contextual Validation: Test model performance with simulated conditions, like using only data from the first half of a season to predict the second half.\n",
    "Challenges and Improvements\n",
    "Challenge: Ensuring the model generalizes well across varied match conditions.\n",
    "Improvement: Implement a context-awareness mechanism where influence scores adjust dynamically based on recent trends (e.g., changes in team form)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
